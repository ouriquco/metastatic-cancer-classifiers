{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datathon: Metastatic Cancer Diagnosis Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Preprocessing and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, ExtraTreesClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "nltk.download('stopwords')\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "def print_metrics(y_test, y_pred):\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1 Score: {f1}')\n",
    "\n",
    "def save_model(file_name, model):\n",
    "    pkl.dump(model, open(file_name, 'wb'))\n",
    "\n",
    "def load_model(file_name):\n",
    "    return pkl.load(open(file_name, 'rb'))\n",
    "\n",
    "def one_hot_encode(df, type):\n",
    "    return pd.get_dummies(df, dtype=type)\n",
    "\n",
    "def random_search_cv(estimator, random_grid):\n",
    "    return RandomizedSearchCV(estimator=estimator, param_distributions = random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs =-1)\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Remove stop words because their frequencies are likely to be larger than more important words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # This effectively removes all special characters without removing other\n",
    "    # characters from different languages\n",
    "    text = p.clean(text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    # remove leading and trailing white spaces with strip()\n",
    "    # remove custom stopwords from text\n",
    "    cleaned_words = []\n",
    "    for word in words:\n",
    "        word = word.strip()\n",
    "\n",
    "        # This will further cleaning some key words by making them uniform\n",
    "        if word in stop_words:\n",
    "            continue\n",
    "        if word == \"malig\":\n",
    "            word = \"malignant\"\n",
    "        elif word == \"neoplm\":\n",
    "            word = \"neoplasm\"\n",
    "        \n",
    "        cleaned_words.append(word)\n",
    "\n",
    "    text = ' '.join(cleaned_words)\n",
    "    # Replace multiple consecutive white spaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def perform_pca_on_model(model_name, model):\n",
    "    pca = PCA()\n",
    "    pipe = Pipeline(steps=[(\"pca\", pca), (model_name, model)])\n",
    "    return pipe, \n",
    "\n",
    "def scale_and_combine_df(df_numeric, df_non_numeric):\n",
    "    x = df_numeric.values\n",
    "    x_scaled = StandardScaler().fit_transform(x)\n",
    "    df_numeric_scaled = pd.DataFrame(x_scaled, columns=df_numeric.columns)\n",
    "    print(len(df_numeric_scaled))\n",
    "    df_final = pd.concat([df_numeric_scaled, df_non_numeric], axis=1)\n",
    "    return df_final\n",
    "\n",
    "def balance_and_sample_data(df):\n",
    "    '''\n",
    "    This function is used to balance the dataset because the classes are heavily skewed. \n",
    "    It also takes random samples from each class. \n",
    "    '''\n",
    "    # Check the counts for each class in the dataset\n",
    "    '''\n",
    "    1    8060\n",
    "    0    4846\n",
    "    '''\n",
    "    print(f'The count for each class before:\\n {df[\"DiagPeriodL90D\"].value_counts()}')\n",
    "    sample_true = df.loc[df['DiagPeriodL90D']==1].sample(n=4846, random_state=42)\n",
    "    sample_false = df.loc[df['DiagPeriodL90D']==0].sample(n=4846, random_state=42)\n",
    "    # After concatenation both classes are balanced at 4846\n",
    "    balanced_df = pd.concat([sample_true,sample_false], axis=0)\n",
    "    \n",
    "    balanced_df = balanced_df.reset_index(drop=True)\n",
    "    print(f'The count for each class after:\\n {balanced_df[\"DiagPeriodL90D\"].value_counts()}')\n",
    "    return balanced_df\n",
    "\n",
    "def preprocess_data(df):\n",
    "    # Don't balance the data set because it reduces model performance\n",
    "    # df_balanced = balance_and_sample_data(df)\n",
    "\n",
    "    # The following columns are dropped because more than 50% of data is missing from these columns.\n",
    "    df.drop(columns=[\"bmi\",\"patient_race\", \"metastatic_first_novel_treatment\", \"metastatic_first_novel_treatment_type\"], inplace=True)\n",
    "    # Save Targets\n",
    "    df_targets = pd.DataFrame(df[\"DiagPeriodL90D\"])\n",
    "    # Divide dataframe into numerical features and non numerical for further processing\n",
    "    df.drop(columns=[\"DiagPeriodL90D\"], inplace=True)\n",
    "    df_non_numeric = df.select_dtypes(exclude=['number'])\n",
    "    df_non_numeric.fillna('CC', inplace=True)\n",
    "    # Clean non numeric column\n",
    "    df_non_numeric.loc[:,\"breast_cancer_diagnosis_desc\"] = df_non_numeric[\"breast_cancer_diagnosis_desc\"].apply(clean_text)\n",
    "    # Use one hot encoding on alpha characters so that the data is interpretable by ML algorithms\n",
    "    df_non_numeric = one_hot_encode(df_non_numeric, float)\n",
    "    df_numeric = df.select_dtypes(include=['number'])\n",
    "    # df_numeric.fillna(df_numeric.mean().round(1), inplace=True)\n",
    "    # Use interpolation because this method will adjacent data to determine the value that should be used to fill empty cells\n",
    "    df_numeric.interpolate(method='polynomial', inplace=True, order=2)\n",
    "    # Scale the data for the algorithms to prevent bias towards features with larger values. Also regularization techniques work better after scaling. \n",
    "    df_final = scale_and_combine_df(df_numeric, df_non_numeric)\n",
    "    print(df_targets.value_counts())\n",
    "    return df_final, df_targets\n",
    "\n",
    "def load_data():\n",
    "    # nltk.download('stopwords')\n",
    "    # df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "    # logistic_regression(df)\n",
    "    # svc(df)\n",
    "    # random_forest_classifier(df)\n",
    "    # extra_trees_classifier(df)\n",
    "    # blender(df)\n",
    "    pass\n",
    "\n",
    "def main():\n",
    "    load_data()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12906, 26)\n",
      "Best parameter (CV score=0.810):\n",
      "{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Accuracy: 0.8152594887683966\n",
      "Precision: 0.790224032586558\n",
      "Recall: 0.9598021026592455\n",
      "F1 Score: 0.8667969840826585\n"
     ]
    }
   ],
   "source": [
    "def svc(df):\n",
    "    # Only use one feature column because CountVectorizer only accepts one column at a time\n",
    "    df.loc[:,\"breast_cancer_diagnosis_desc\"] = df[\"breast_cancer_diagnosis_desc\"].apply(clean_text)\n",
    "    df_features = df[\"breast_cancer_diagnosis_desc\"]\n",
    "    df_targets = df[\"DiagPeriodL90D\"]\n",
    "    \n",
    "    # CountVectorize takes the raw frequency of each word found in each document and returns a vector matrix\n",
    "    vectorized_features = CountVectorizer()\n",
    "    df_features = vectorized_features.fit_transform(df_features)\n",
    "    print(df_features.shape)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features, df_targets, test_size=0.2, random_state=42)\n",
    "    SVM = svm.SVC()\n",
    "    \n",
    "    # Random search is used before grid search to reduce search space\n",
    "    # Then we use grid search with the narrowed results from Random search\n",
    "    # SVM = svc_random_search(SVM, X_train, y_train)\n",
    "    SVM = svc_grid_search(SVM, X_train, y_train)\n",
    "    y_pred = SVM.predict(X_test)\n",
    "\n",
    "    print_metrics(y_test, y_pred)\n",
    "    save_model('saved_models/svm.sav',SVM)\n",
    "\n",
    "def svc_random_search(SVM, X_train, y_train):\n",
    "    # These are the latest results from RandomSearchCV\n",
    "    '''\n",
    "    Best parameter (CV score=0.810):\n",
    "    {'kernel': 'rbf', 'gamma': 0.1, 'C': 10}\n",
    "    '''\n",
    "    param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'kernel': ['linear', 'rbf', 'poly'],\n",
    "              'gamma': np.logspace(-3, 2, num=6)}\n",
    "    svc_model = random_search_cv(SVM,param_grid)\n",
    "    svc_model.fit(X_train, y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % svc_model.best_score_)\n",
    "    print(svc_model.best_params_)\n",
    "    return svc_model\n",
    "\n",
    "def svc_grid_search(SVM, X_train, y_train):\n",
    "    # These are the latest results from GridSearchCV\n",
    "    '''\n",
    "    Best parameter (CV score=0.810):\n",
    "    {'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}\n",
    "    '''\n",
    "    param_grid = {'C': [1, 10, 20],\n",
    "              'kernel': ['rbf'],\n",
    "              'gamma': [0.1,.5,1,10]}\n",
    "    \n",
    "    svm_model = GridSearchCV(SVM, param_grid, n_jobs=-1)\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % svm_model.best_score_)\n",
    "    print(svm_model.best_params_)\n",
    "    return svm_model\n",
    "\n",
    "# Run SVM classifier\n",
    "# Import data for training \n",
    "\n",
    "df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "svc(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12906\n",
      "DiagPeriodL90D\n",
      "1                 8060\n",
      "0                 4846\n",
      "dtype: int64\n",
      "Accuracy: 0.8140975987606507\n",
      "Precision: 0.7899031106578276\n",
      "Recall: 0.9579468150896723\n",
      "F1 Score: 0.8658468418110675\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(df):\n",
    "    '''\n",
    "    Best parameter (CV score=0.810):\n",
    "    {'C': 10, 'max_iter': 100, 'multi_class': 'auto', 'solver': 'liblinear'}\n",
    "    '''\n",
    "    df_features, df_targets = preprocess_data(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features.values, df_targets.values.ravel(), test_size=0.2, random_state=42)\n",
    "    logistic_model = LogisticRegression(solver=\"liblinear\", penalty='l2',C=10, multi_class='auto',max_iter=100, tol=0.1)\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    # logistic_model = logistic_regression_grid_search(logistic, X_train, y_train)\n",
    "    y_pred = logistic_model.predict(X_test)\n",
    "    print_metrics(y_test, y_pred)\n",
    "    save_model('saved_models/logistic_regression.sav',logistic_model)\n",
    "\n",
    "def logistic_regression_grid_search(logistic, X_train, y_train):\n",
    "    '''\n",
    "    Best parameter (CV score=0.810):\n",
    "    {'C': 10, 'max_iter': 100, 'multi_class': 'auto', 'solver': 'liblinear'}\n",
    "    '''\n",
    "    param_grid = {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],  \n",
    "        'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
    "        'multi_class': ['auto', 'ovr'],\n",
    "        'max_iter': [100, 200, 500, 1000],\n",
    "    }\n",
    "\n",
    "    logistic_model = GridSearchCV(logistic, param_grid, n_jobs=-1)\n",
    "    logistic_model.fit(X_train, y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % logistic_model.best_score_)\n",
    "    print(logistic_model.best_params_)\n",
    "    return logistic_model\n",
    "\n",
    "# Run logistic regression classifier\n",
    "df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "logistic_regression(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12906\n",
      "DiagPeriodL90D\n",
      "1                 8060\n",
      "0                 4846\n",
      "dtype: int64\n",
      "Accuracy: 0.8164213787761425\n",
      "Precision: 0.7890743550834598\n",
      "Recall: 0.9647495361781077\n",
      "F1 Score: 0.8681135225375627\n"
     ]
    }
   ],
   "source": [
    "def random_forest_classifier(df):\n",
    "    df_features, df_targets = preprocess_data(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features.values, df_targets.values.ravel(), test_size=0.2, random_state=42)\n",
    "    rnd_clf = RandomForestClassifier(n_estimators=500, criterion=\"gini\",  max_leaf_nodes=16, random_state=42, n_jobs =-1)\n",
    "    # rnd_clf_model = random_forest_classifier_pipe_random_search(rnd_clf, X_train, y_train)\n",
    "    rnd_clf.fit(X_train, y_train)\n",
    "    y_pred=  rnd_clf.predict(X_test)\n",
    "    print_metrics(y_test, y_pred)\n",
    "    save_model('saved_models/random_forest_classifier.sav',rnd_clf)\n",
    "\n",
    "def random_forest_classifier_pipe_random_search(rnd_clf, X_train, y_train):\n",
    "    n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "    max_features = ['auto', 'sqrt']\n",
    "    max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "    max_depth.append(None)\n",
    "    min_samples_split = [2,5,10]\n",
    "    min_samples_leaf = [1,2,4]\n",
    "    bootstrap = [True, False]\n",
    "\n",
    "    random_grid = { 'random_forest__n_estimators': n_estimators,\n",
    "                    'random_forest__max_features': max_features,\n",
    "                    'random_forest__max_depth': max_depth,\n",
    "                    'random_forest__min_samples_split': min_samples_split,\n",
    "                    'random_forest__min_samples_leaf': min_samples_leaf,\n",
    "                    'random_forest__bootstrap': bootstrap}\n",
    "\n",
    "    pipe = Pipeline(steps=[(\"pca\",PCA()),(\"random_forest\", rnd_clf)])\n",
    "    rnd_clf_model = random_search_cv(pipe, random_grid)\n",
    "    rnd_clf_model.fit(X_train, y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % rnd_clf_model.best_score_)\n",
    "    print(rnd_clf_model.best_params_)\n",
    "    return rnd_clf_model\n",
    "\n",
    "# Run random forest classifier \n",
    "# This algorithm achieved the second best scores. I tried using a pipeline with PCA, hoping to reduce the dimensionality.\n",
    "# However, the model did not perform as well. Therefore, I did not use it in the model's final computation.  \n",
    "df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "random_forest_classifier(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12906\n",
      "DiagPeriodL90D\n",
      "1                 8060\n",
      "0                 4846\n",
      "dtype: int64\n",
      "Accuracy: 0.8171959721146398\n",
      "Precision: 0.789873417721519\n",
      "Recall: 0.9647495361781077\n",
      "F1 Score: 0.8685968819599108\n"
     ]
    }
   ],
   "source": [
    "def extra_trees_classifier(df):\n",
    "    df_features, df_targets = preprocess_data(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features.values, df_targets.values.ravel(), test_size=0.2, random_state=42)\n",
    "\n",
    "    extra_trees_clf = ExtraTreesClassifier(n_estimators=500, criterion=\"gini\", max_features='sqrt', min_samples_split=10, min_samples_leaf=4, max_leaf_nodes=16, random_state=42, n_jobs =-1)\n",
    "    extra_trees_clf.fit(X_train, y_train)\n",
    "    # extra_trees_clf = extra_trees_classifier_grid_search(X_train, y_train)\n",
    "    y_pred=  extra_trees_clf.predict(X_test)\n",
    "    print_metrics(y_test, y_pred)\n",
    "    save_model('saved_models/extra_trees_classifier.sav',extra_trees_clf)\n",
    "\n",
    "def extra_trees_classifier_grid_search(X_train, y_train):\n",
    "    '''\n",
    "    accuracy = .8168\n",
    "    Best parameter (CV score=0.810):\n",
    "    {'bootstrap': True, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 500}\n",
    "    '''\n",
    "    param_grid = {\n",
    "            'n_estimators': [150, 300, 500],  \n",
    "            'max_depth': [None, 5, 10],       \n",
    "            'min_samples_split': [2, 5, 10],  \n",
    "            'min_samples_leaf': [1, 2, 4],    \n",
    "            'max_features': ['auto', 'sqrt', 'log2'],  \n",
    "            'bootstrap': [True, False]      \n",
    "        }   \n",
    "    extra_trees_clf = GridSearchCV(ExtraTreesClassifier(), param_grid=param_grid, n_jobs=-1)\n",
    "    extra_trees_clf.fit(X_train, y_train)\n",
    "    print(\"Best parameter (CV score=%0.3f):\" % extra_trees_clf.best_score_)\n",
    "    print(extra_trees_clf.best_params_)\n",
    "    return extra_trees_clf\n",
    "\n",
    "\n",
    "# Run extra trees classifier\n",
    "# Overall I believe this model achieved the highest accuracy because it is an ensemble model that offers more randomness than \n",
    "# RandomForestClassifier because splits are chosen completely randomly from all features. \n",
    "df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "extra_trees_classifier(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8152594887683966\n",
      "Precision: 0.790224032586558\n",
      "Recall: 0.9598021026592455\n",
      "F1 Score: 0.8667969840826585\n"
     ]
    }
   ],
   "source": [
    "def blender(df):\n",
    "\n",
    "    # # Here is data preprocessing for two models\n",
    "    # df_features, df_targets = preprocess_data(df)\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(df_features.values, df_targets.values.ravel(), test_size=0.2, random_state=42)\n",
    "\n",
    "    # Data preprocessing for SVM \n",
    "    df.loc[:, \"breast_cancer_diagnosis_desc\"] = df[\"breast_cancer_diagnosis_desc\"].apply(clean_text)\n",
    "    df_features = df[\"breast_cancer_diagnosis_desc\"]\n",
    "    df_targets = df[\"DiagPeriodL90D\"]\n",
    "    \n",
    "    vectorized_features = CountVectorizer()\n",
    "    df_features = vectorized_features.fit_transform(df_features)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_features, df_targets, test_size=0.2, random_state=42)\n",
    "\n",
    "    stacking_clf = StackingClassifier(\n",
    "        estimators=[\n",
    "            ('lr', LogisticRegression(solver=\"liblinear\", max_iter=100, tol=0.1, random_state=42, C=10, multi_class='auto')),\n",
    "            ('rf', RandomForestClassifier(n_estimators=500, criterion=\"gini\",  max_leaf_nodes=16, random_state=42, n_jobs =-1)),\n",
    "            ('svc', svm.SVC(probability=True, kernel='rbf', gamma=0.1, C=10,random_state=42))\n",
    "        ]\n",
    "    )\n",
    "    stacking_clf.fit(X_train, y_train)\n",
    "    y_pred = stacking_clf.predict(X_test)\n",
    "    print_metrics(y_test, y_pred)\n",
    "    save_model('saved_models/stacking_classifier.sav',stacking_clf)\n",
    "\n",
    "# Run stacking classifier \n",
    "# This model did not perform as well as the extra trees classifier becuase I did not have enough time to optimize it. \n",
    "df = pd.read_csv(\"training_wids2024C1.csv\")\n",
    "blender(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Géron, A. (2023). Hands-on machine learning with scikit-learn, keras and tensorflow: Concepts, tools, and techniques to build Intelligent Systems. O’Reilly Media, Inc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
